{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "HW4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMV1ThXoLxBD"
      },
      "source": [
        "# HW4: RNNs and GANs\n",
        "\n",
        "\n",
        "Designed by Anil Kag drawing from prior work by Samarth Mishra, Kun He, Xide Xia, Kubra Cilingir, Vijay Thakkar, Ali Siahkamari, and Brian Kulis.\n",
        "\n",
        "\n",
        "This assignment will introduce you to \n",
        "\n",
        "1. Understanding state transitions in Vanilla RNNs by solving a simple sequential addition task. [20 points]\n",
        "\n",
        "2. Building a sentiment classifier using LSTMs. This task is based on a popular IMDB review dataset. [40 points]\n",
        "\n",
        "3. Introduce Generative Adversarial Learning building blocks. [40 points]\n",
        "\n",
        "4. Use above blocks to change hairstyles of popular celebrities available in the CelebA dataset. [60 points]\n",
        "\n",
        "**NOTE** : Problem 3 and 4 are time consuming, not just from code perspective but it takes some effort in training to get reasonable GAN performance (you should expect the training to take roughly 10 hours or more depending on the complexity you choose), so **PLEASE START EARLY.**\n",
        "\n",
        "## Preamble\n",
        "\n",
        "We recommend using GPU as the primary compute for these problems. For the first two tasks, we do not expect you to require too much GPU compute time (efficient code should finish within 20mins on a GPU like K80). For the GAN problems, we expect longer run time (specially the problem 4 should take more than 6 hours to get a reasonable generator).\n",
        "\n",
        "## Environment setup\n",
        "\n",
        "We provide you the datasets required for this assignment as shared folders in both SCC (`'/projectnb/ec523/anilkag/datasets/'`) and [Google Drive](https://drive.google.com/drive/folders/1EFJKm8mZk6GPrN2BRuadL_HRg0STb7V0?usp=sharing).\n",
        "\n",
        "\n",
        "Below we describe the environment setup steps depending on your preferrence. Through [(SCC)](https://www.bu.edu/tech/support/research/computing-resources/scc/) you can get access to GPU compute, below we describe the steps for setting up a tunnel so that you can open a Jupyter notebook on your end.\n",
        "\n",
        "If you intend to use Google colab, feel free to skip the below steps. Remember, Google colab only provides you a preemptive access to a GPU, so technically you could loose access to a GPU in the middle of your experiments. Ideally, it will allow you access to a GPU for 12 hours (free quota). Also, since its a shared resource, be respectful of the Google colab usage policy. \n",
        "\n",
        "## SCC Configuration\n",
        "\n",
        "For this assignment, we recommend that you use the shared computing cluster [(SCC)](https://www.bu.edu/tech/support/research/computing-resources/scc/). Each of you has an account on the SCC and can login using (more info on the [quick start guide](https://www.bu.edu/tech/support/research/system-usage/scc-quickstart/)):\n",
        "```\n",
        "ssh <bu_loginname>@scc1.bu.edu\n",
        "```\n",
        "\n",
        "[comment]: #   ( Copy `HW4_datasets.zip` to the SCC, extract and change working directory to the extracted folder. )\n",
        "\n",
        "Here, we provide some instructions to start a jupyter notebook or jupyter lab server on the SCC. More detailed instructions can be found on the SCC's info website (linked above). These instructions are also fairly easy to find with a google search.\n",
        "You can then request an interactive session on a compute node using:\n",
        "```\n",
        "qrsh -pe omp <num_cpu_cores> -l gpus=<num_gpus_per_cpu_core> -l gpu_c=<gpu_compute_capacity>\n",
        "```\n",
        "Recommended values for the parameters in <> above are 2, 0.5 and 3.5 repectively, which will assign you 2 cpu cores, 1 gpu of compute capacity at least 3.5 for an interactive session of 12 hours. We also have access to some GPUs with compute capacity 6.0 (but they may be somewhat limited in numbers), if you want to explore these GPUs, feel free to request with gpu_c=6.0. \n",
        "\n",
        "Now that you are on a compute node, you can load modules you need as:\n",
        "```\n",
        "module load cuda/10.1\n",
        "module load python3/3.6.9\n",
        "module load pytorch/1.3\n",
        "```\n",
        "Running a jupyter server (you can replace the following with `jupyter notebook` for the old interface):\n",
        "```\n",
        "jupyter lab --no-browser --ip=0.0.0.0 --port=8888\n",
        "```\n",
        "To access the jupyter interface on a browser on your personal machine, use ssh port forwarding as,\n",
        "```\n",
        "ssh -N -f -L 8889:<scc-compute-node>:8888 <bu_loginname>@scc1.bu.edu\n",
        "```\n",
        "where `<scc_compute_node>` is the compute node on the scc that you have access to through your interactive session (e.g. `scc-k02`). Open a browser and the interface should be accessible at `localhost:8889`. The ports `8888` and `8889` can be changed for something else. If these ports are already in use, feel free to use unused port numbers in the above command.\n",
        "\n",
        "### Assignment Shared Data path\n",
        "\n",
        "If you are using your local machine or SSC, please update the python variable `EXPERIMENTS_DIRECTORY` and `DATA_DIR`, prepopulated below to the correct folder where you have stored the files provided for this assignment.\n",
        "\n",
        "Note that \n",
        "1. `EXPERIMENTS_DIRECTORY` : refers to the folder where your experiment results, models, logs and the sample generated will be stored. Please update the path to a folder you have write access. For SCC, it is recommended that you create a folder, ` '/projectnb/ec523/<USER-NAME>/experiments'`, change `<USER-NAME>` to your scc username. For Google colab, use any folder you have write access.\n",
        "\n",
        "2. `DATA_DIR` : refers to the folder where the datasets required for this assignment are stored (For scc, I've already copied the datasets to a folder ( `'/projectnb/ec523/anilkag/datasets/'` ) where everyone has read accesss. For Google colab, I've shared the dataset at the following directory, `https://drive.google.com/drive/folders/1EFJKm8mZk6GPrN2BRuadL_HRg0STb7V0?usp=sharing`, please update this variable according the path that your BU Google drive account shows. )\n",
        "\n",
        "## Google Colab Configuration\n",
        "\n",
        "No special setup is required (just ensure that the runtime is setup to utilize a GPU).\n",
        "\n",
        "### Assignment Shared Data path\n",
        "\n",
        "If you are using Google Colab for the experiments, it is easy to link your BU google drive account for storage purposes.\n",
        "\n",
        "1. First step is to give access permissions to the google colab. Following commands perform the authentication (run these two lines of code, it'll provide you the link for authentication, copy & paste the authorization code in the fill in box provided by the script.)\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "```\n",
        "\n",
        "2. Finding the right folder path for experiments. Similar to the ```ls``` command in linux, the following command lists the content of the drive (use this to choose your base experiment directory)\n",
        "```\n",
        "ls \"/content/gdrive/My Drive\"\n",
        "```\n",
        "\n",
        "3. Please copy all the data provided for this assignment  to the experiment directory, say in the location ```\"/content/gdrive/My Drive/Experiments-Deep-Learning-HW4\"``` (If you change the path, please update the same in the script below.)\n",
        "```\n",
        "ls \"/content/gdrive/My Drive/Experiments-Deep-Learning-HW4\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNq-hdDoOrNE"
      },
      "source": [
        "# Change this part to locate the correct directory where dataset resides\n",
        "use_colab = True\n",
        "\n",
        "if use_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    #!ls \"/content/gdrive/My Drive/Datasets/shared/HW4_shared_files\"\n",
        "\n",
        "    ## Update the experiments directory\n",
        "    EXPERIMENTS_DIRECTORY = '/content/gdrive/My Drive/Datasets/shared/experiments/'\n",
        "    DATA_DIRECTORY = '/content/gdrive/My Drive/Datasets/shared/HW4_shared_files/'\n",
        "    CELEBA_GOOGLE_DRIVE_PATH = DATA_DIRECTORY + 'celeba_attributes_images.hdf5'\n",
        "    IMDB_REVIEWS_FILE_PATH = DATA_DIRECTORY + 'data/'\n",
        "else:\n",
        "    ## Update the experiments directory\n",
        "    EXPERIMENTS_DIRECTORY = '/projectnb/ec523/anilkag/experiments'\n",
        "    DATA_DIRECTORY = '/projectnb/ec523/anilkag/datasets/'\n",
        "    CELEBA_GOOGLE_DRIVE_PATH = DATA_DIRECTORY + 'celeba_attributes_images.hdf5'\n",
        "    IMDB_REVIEWS_FILE_PATH = DATA_DIRECTORY + 'data/'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTdfYxkGOtzW"
      },
      "source": [
        "### Modules needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8sqPyyXOtKC"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "import datetime\n",
        "import time\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.utils import data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torchvision import transforms as T\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "from torch.backends import cudnn\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "# manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "np.random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q-Or0bGOxLk"
      },
      "source": [
        "\n",
        "\n",
        "## Q1. RNN Example [20 points]\n",
        "\n",
        "In this example we train an RNN to solve a simple addition task presented in a sequential manner. The input data of the dataset consists of two rows. The first row contains random float numbers between 0 and 1; the second row are all zeros, expect two randomly chosen locations being marked as 1. The corresponding output label is a float number summing up two numbers in the first row of the input data where marked as 1 in the second row. The length of the row T is the length of the input sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZiup1gKeeYb"
      },
      "source": [
        "\n",
        "<img src='https://minpy.readthedocs.io/en/latest/_images/adding_problem.png' style=\"width: 600px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F10VFlkpeeIQ"
      },
      "source": [
        "We will first implement a vanilla RNN that stores the hidden state $h_t$ as a summary of the sequential input seen up to timestep $t$.\n",
        "\n",
        "1. We assume that $\\{ x_t \\}^T_{t=1}$ is the sequential input data to an RNN.\n",
        "\n",
        "2. A vanilla RNN maintains $h_t = \\phi( U h_{t-1} + W x_{t} + b )$ where $h_{t-1}$ is the hidden state upto timestep $t-1$ and $x_t$ is the current input. \n",
        "\n",
        "3. The parameters $\\{ U, W, b \\}$ are learnt by training the RNN on the given training data through back-propagation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEXroZpoiGF9"
      },
      "source": [
        "To start with, the following code segment (after the next cell, which is meant for imports) generates the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLKkc-YiddIE"
      },
      "source": [
        "def adding_problem_generator(N, seq_len=6, high=1, number_of_ones=2): \n",
        "    X_num = np.random.uniform(low=0, high=high, size=(N, seq_len, 1))\n",
        "    X_mask = np.zeros((N, seq_len, 1))\n",
        "    Y = np.ones((N, 1))\n",
        "    for i in range(N):\n",
        "        # Default uniform distribution on position sampling\n",
        "        positions1 = np.random.choice(np.arange(math.floor(seq_len/2)), size=math.floor(number_of_ones/2), replace=False)\n",
        "        positions2 = np.random.choice(np.arange(math.ceil(seq_len/2), seq_len), size=math.ceil(number_of_ones/2), replace=False)\n",
        "\n",
        "        positions = []\n",
        "        positions.extend(list(positions1))\n",
        "        positions.extend(list(positions2))\n",
        "        positions = np.array(positions)\n",
        "\n",
        "        X_mask[i, positions] = 1        \n",
        "        Y[i, 0] = np.sum(X_num[i, positions])\n",
        "    X = np.append(X_num, X_mask, axis=2)\n",
        "    return torch.FloatTensor(X), torch.FloatTensor(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXTp4R6Pj3v3",
        "outputId": "0802545f-5e52-4117-e4c0-db1fd916311f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#tmpX, tmpY = adding_problem_generator(1, seq_len=6, high=1, number_of_ones=2)\n",
        "#print('First data point : ')\n",
        "#print(tmpX[0])\n",
        "#print(tmpY[0]) \n",
        "print('Uncomment the above function calls to see the input data point and the output ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uncomment the above function calls to see the input data point and the output \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwcyYCD0JWCt"
      },
      "source": [
        "### Q1.1 (10 points)\n",
        "Write the transition function that takes the previous hidden state $h_{t-1}$ and current observation $x_t$ as input and outputs the next hidden state $h_t$ using the parameter matrices. (HINT: initialize the bias to be close to 0.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGoEMMlYsuuC"
      },
      "source": [
        "class VanillaRNNCell( nn.Module ):\n",
        "  def __init__(self, input_size, hidden_size, nonlinearity='tanh'):\n",
        "    super(VanillaRNNCell, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.nonlinearity = nonlinearity\n",
        "\n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    #####     Declare your parameters here (U, W, b)\n",
        "    #####     Use nn.Parameter and shapes given above for declaration\n",
        "    raise NotImplementedError\n",
        "    ################################################\n",
        "  \n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    #####     Initialize your parameters (if not then it'll use default initialization)\n",
        "    #####     Use nn.init functions \n",
        "    raise NotImplementedError\n",
        "    ################################################\n",
        "\n",
        "  def forward(self, x, h):\n",
        "    if h is None:\n",
        "      h = x.new_zeros(x.shape[0], self.hidden_size)\n",
        "\n",
        "    ## \n",
        "    ## [CODE] Write the transition step and return new hidden state\n",
        "    ##        h_t = nonlinearity( U * h_{t-1} + W * x_t + b )\n",
        "    ##  \n",
        "    ##        Use torch.relu, torch.tanh to differentiate the two case           \n",
        "    if self.nonlinearity == 'relu':\n",
        "      raise NotImplementedError\n",
        "    elif self.nonlinearity == 'tanh':\n",
        "      raise NotImplementedError\n",
        "    else:\n",
        "      raise RuntimeError(\"Unknown nonlinearity: {}\".format(self.nonlinearity))\n",
        "\n",
        "    return h\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJJg89aulxl8"
      },
      "source": [
        "Below is the boiler plate code which takes as input an RNN implementation and trains a simple network to solve the Addition task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9IB_HcGjVm1"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, rec_net):\n",
        "        super(Model, self).__init__()\n",
        "        self.rnn = rec_net\n",
        "        self.lin = nn.Linear(hidden_size, 1) \n",
        "        self.loss_func = nn.MSELoss()\n",
        "\n",
        "        nn.init.xavier_normal_(self.lin.weight)\n",
        "\n",
        "    def forward(self, x, y):        \n",
        "        loss = 0\n",
        "        hidden = None\n",
        "\n",
        "        for i in range(len(x)):\n",
        "            hidden = self.rnn.forward(x[i], hidden)\n",
        "\n",
        "        out = self.lin(hidden)           \n",
        "        loss += self.loss_func(out, y.squeeze(1).t())\n",
        "        return loss\n",
        "\n",
        "def train_model(net, optimizer, batch_size, n_steps, c_length=20):\n",
        "    accs = []\n",
        "    losses = []\n",
        "    rec_nets = []\n",
        "    first_hid_grads = []\n",
        "    \n",
        "    for i in range(n_steps):        \n",
        "        s_t = time.time()\n",
        "        x,y = adding_problem_generator(batch_size, seq_len=c_length, number_of_ones=2)        \n",
        "        if CUDA:\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "        x = x.transpose(0, 1)\n",
        "        y = y.transpose(0, 1)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss = net.forward(x, y)\n",
        "        loss_act = loss\n",
        "        \n",
        "        loss.backward()        \n",
        "        losses.append(loss_act.item())\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if i%200 == 0:\n",
        "          print('Update {}, Time for Update: {} , Average Loss: {}'\n",
        "              .format(i +1, time.time()- s_t, loss_act.item() ))\n",
        "    \n",
        "    print(\"Average loss: \", np.mean(np.array(losses)))\n",
        "    return losses\n",
        "\n",
        "def run_addition_task( rnn, rnn_cell_name ):\n",
        "  print('\\n\\nWill solve using RNN=', rnn_cell_name)\n",
        "  net = Model(rnn)\n",
        "  if CUDA:\n",
        "    net = net.cuda()\n",
        "    net.rnn = net.rnn.cuda()\n",
        "\n",
        "  optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "  losses = train_model(net, optimizer, batch_size, n_steps = 1000)\n",
        "  return losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsASB2nKwCiN"
      },
      "source": [
        "### Q1.2 (10 points)\n",
        "Utilize the VanillaRNNCell written above and run the train code resulting in the convergence plot which shows the progress on this learning task. (As a reference, you should say that the problem is solved when the loss becomes less than 0.001) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3U0bLxDddNd"
      },
      "source": [
        "input_size = 2\n",
        "hidden_size = 128\n",
        "batch_size = 100\n",
        "\n",
        "from torch.nn import RNNCell, GRUCell \n",
        "\n",
        "################################################\n",
        "##### TODO CODE HERE\n",
        "#####     Invoke RNNCells written above with both tanh and relu nonlinearities\n",
        "#####     Also, invoke a GRUCell in order to compare the performance \n",
        "vanilla_rnn_tanh = raise NotImplementedError\n",
        "vanilla_rnn_relu = raise NotImplementedError\n",
        "gru = raise NotImplementedError\n",
        "################################################\n",
        "\n",
        "\n",
        "## This collects the losses and generates plot for visualization\n",
        "data = {}\n",
        "data['VanillaRNN(tanh)'] = run_addition_task(vanilla_rnn_tanh, 'VanillaRNN(tanh)' )\n",
        "data['VanillaRNN(relu)'] = run_addition_task(vanilla_rnn_relu, 'VanillaRNN(relu)' )\n",
        "data['GRU'] = run_addition_task( gru, 'GRU' )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBrWCqeR2Rs-"
      },
      "source": [
        "Run the below written script to generate the convergence plot (shows the progress on the learning task). Please leave the graph in the submission pdf so that we can evaluate the convergence (not doing so will result in 5 point deduction.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2we7-CTB1mUm"
      },
      "source": [
        "legends = []\n",
        "for label in data.keys():\n",
        "    losses = data[label]\n",
        "    plt.plot(1 + np.arange(len(losses)),  losses )\n",
        "    legends.append(label)\n",
        "    \n",
        "plt.legend(legends, loc='upper right')\n",
        "plt.title('Training Error ')\n",
        "plt.xlabel('Training Steps')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.ylim(0.0, 0.5)\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-rhdU-tO_ci"
      },
      "source": [
        "## Q2: Movie Review Sentiment Classification using an LSTM [40 points]\n",
        "\n",
        "In this part you will train a sentiment classifier using LSTM that predicts whether a movie review is positive or negative.  \n",
        "\n",
        "### IMDB Reviews\n",
        "\n",
        "This dataset contains reviews taken from the IMDB website for many popular movies. This has been curated to avoid spurious characters and contains only english reviews. \n",
        "\n",
        "Below we provide an interface which loads the training IMDB review set and pre-process it in order to create a vocabulary, as well as split this train set into train, test, and validation set for the purposes of this experiment. Do not modify this `ReviewDataset` interface.\n",
        "\n",
        "Note that we also provide two sample positive and negative reviews which are used later to test whether your learnt LSTM can detective the positive and negative sentiments attached to the reviews. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtEMGdsGPCra"
      },
      "source": [
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "class ReviewDataset():\n",
        "    def __init__(self, seq_length = 200):\n",
        "        # Maximum sequence length for reviews\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        self.reviews_file = IMDB_REVIEWS_FILE_PATH + 'reviews.txt' \n",
        "        self.labels_file  = IMDB_REVIEWS_FILE_PATH + 'labels.txt'  \n",
        "\n",
        "        # Read the reviews and labels \n",
        "        self.read_reviews_labels()\n",
        "\n",
        "        # Create a vocabulary from the reviews\n",
        "        self.create_vocabulary()\n",
        "\n",
        "        # Convert the reviews into integer tokens using vocab\n",
        "        self.tokenize_reviews()\n",
        "\n",
        "        self.create_train_test_valid_split()\n",
        "\n",
        "    def read_reviews_labels(self):\n",
        "        print('Reading reviews and labels from the data folder.')\n",
        "\n",
        "        # read data from text files\n",
        "        with open(self.reviews_file, 'r') as f:\n",
        "            self.reviews = f.read()\n",
        "\n",
        "        with open(self.labels_file, 'r') as f:\n",
        "            self.labels = f.read()\n",
        "\n",
        "        # lowercase, standardize\n",
        "        self.reviews = self.reviews.lower() \n",
        "\n",
        "        print('Will remove punctuation = ', punctuation)\n",
        "\n",
        "        # get rid of punctuation\n",
        "        all_text = ''.join([c for c in self.reviews if c not in punctuation])\n",
        "\n",
        "        # split by new lines and spaces\n",
        "        self.reviews_split = all_text.split('\\n')\n",
        "\n",
        "        # 1=positive, 0=negative label conversion\n",
        "        self.labels_split = self.labels.split('\\n')\n",
        "        self.encoded_labels = np.array([1 if label == 'positive' else 0 for label in self.labels_split])\n",
        "\n",
        "    def create_vocabulary(self):\n",
        "        # Gather all text from the Corpus\n",
        "        all_text = ' '.join(self.reviews_split)\n",
        "\n",
        "        # create a list of words\n",
        "        words = all_text.split()\n",
        "\n",
        "        # Build a dictionary that maps words to integers\n",
        "        counts = Counter(words)\n",
        "        self.vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "        self.vocab_to_int = {word: ii for ii, word in enumerate(self.vocab,1)} \n",
        "\n",
        "        # stats about vocabulary\n",
        "        print('Unique words: ', len((self.vocab_to_int)))  # should ~ 74000+\n",
        "\n",
        "    def pad_features(self, reviews_ints, seq_length):\n",
        "        ''' Return features of review_ints, where each review is padded with 0's \n",
        "            or truncated to the input seq_length.\n",
        "        '''\n",
        "        ## getting the correct rows x cols shape\n",
        "        features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "        \n",
        "        ## for each review, I grab that review\n",
        "        for i, row in enumerate(reviews_ints):\n",
        "          features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "        \n",
        "        return features\n",
        "\n",
        "    def tokenize_reviews(self):\n",
        "        ## use the dict to tokenize each review in reviews_split\n",
        "        ## store the tokenized reviews in reviews_ints\n",
        "        self.reviews_ints = []\n",
        "        for review in self.reviews_split:\n",
        "            self.reviews_ints.append([self.vocab_to_int[word] for word in review.split()])\n",
        "\n",
        "        ## get any indices of any reviews with length 0\n",
        "        non_zero_idx = [ii for ii, review in enumerate(self.reviews_ints) if len(review) != 0]\n",
        "\n",
        "        # remove 0-length review with their labels\n",
        "        self.reviews_ints = [self.reviews_ints[ii] for ii in non_zero_idx]\n",
        "        self.encoded_labels = np.array([self.encoded_labels[ii] for ii in non_zero_idx])\n",
        "        self.reviews_split = [self.reviews_split[ii] for ii in non_zero_idx]\n",
        "        self.labels_split = [self.labels_split[ii] for ii in non_zero_idx]\n",
        "\n",
        "        print('Number of reviews after removing outliers: ', len(self.reviews_ints))\n",
        "\n",
        "        self.features = self.pad_features(self.reviews_ints, self.seq_length)\n",
        "\n",
        "        ## test statements - do not change - ##\n",
        "        assert len(self.features)==len(self.reviews_ints), \"Your features should have as many rows as reviews.\"\n",
        "        assert len(self.features[0])==self.seq_length, \"Each feature row should contain seq_length values.\"\n",
        "\n",
        "        # print tokens in first review\n",
        "        print('Raw review: \\n', self.reviews_split[0])\n",
        "        print('Tokenized review: \\n', self.reviews_ints[0])\n",
        "        print('Raw label: ', self.labels_split[0])\n",
        "        print('Encoded label: ', self.encoded_labels[0])\n",
        "\n",
        "    def tokenize_review(self, review):\n",
        "        # lowercase\n",
        "        review = review.lower() \n",
        "\n",
        "        # get rid of punctuatuon\n",
        "        text = ''.join([c for c in review if c not in punctuation])\n",
        "        \n",
        "        # splitting by spaces\n",
        "        words = text.split()\n",
        "        \n",
        "        # tokens\n",
        "        tokens = []\n",
        "        tokens.append([self.vocab_to_int[word] for word in words])\n",
        "        #print(tokens)\n",
        "        \n",
        "        # sequence padding\n",
        "        features = self.pad_features(tokens, self.seq_length)\n",
        "        #print(features)\n",
        "\n",
        "        # test conversion to tensor and pass it to model\n",
        "        feature_tensor = torch.from_numpy(features)\n",
        "        #print(feature_tensor.size())\n",
        "\n",
        "        return feature_tensor\n",
        "\n",
        "    def get_positive_and_negative_reviews(self):\n",
        "        # negative test review\n",
        "        review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'\n",
        "\n",
        "        # positive test review\n",
        "        review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'\n",
        "\n",
        "        #features_pos = tokenize_review(review_pos)\n",
        "        #features_neg = tokenize_review(review_neg)\n",
        "        #return review_pos, review_neg, features_pos, features_neg\n",
        "        return review_pos, review_neg\n",
        "\n",
        "    def create_train_test_valid_split(self):\n",
        "        split_frac = 0.8\n",
        "\n",
        "        # split data into training, validation, and test data (features and labels, x and y)\n",
        "        split_idx = int(len(self.features)*0.8)\n",
        "        self.train_x, remaining_x = self.features[:split_idx], self.features[split_idx:]\n",
        "        self.train_y, remaining_y = self.encoded_labels[:split_idx], self.encoded_labels[split_idx:]\n",
        "\n",
        "        test_idx = int(len(remaining_x)*0.5)\n",
        "        self.val_x, self.test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "        self.val_y, self.test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "        # print out the shapes of your resultant feature data\n",
        "        print(\"\\t\\t\\tFeatures Shapes:\")\n",
        "        print(\"Train set: \\t\\t{}\".format(self.train_x.shape),\n",
        "              \"\\nValidation set: \\t{}\".format(self.val_x.shape),\n",
        "              \"\\nTest set: \\t\\t{}\".format(self.test_x.shape))\n",
        "\n",
        "    def get_data_loaders(self, batch_size = 50):\n",
        "        # create Tensor datasets\n",
        "        train_data = TensorDataset(torch.from_numpy(self.train_x), torch.from_numpy(self.train_y))\n",
        "        valid_data = TensorDataset(torch.from_numpy(self.val_x), torch.from_numpy(self.val_y))\n",
        "        test_data = TensorDataset(torch.from_numpy(self.test_x), torch.from_numpy(self.test_y))\n",
        "\n",
        "        # make sure to SHUFFLE your data\n",
        "        train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "        valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "        test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "        return train_loader, valid_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vesMv5aO_h7"
      },
      "source": [
        "### Q2.1 Instantiate dataset and gather data loaders [5 points]\n",
        "\n",
        "We need to get hold of the data loaders for train, valid, and test splits. The data handling utility is available in the `ReviewDataset` class. Please invoke the appropriate functions to get hold of the corresponding data loaders. \n",
        "\n",
        "After setting up the data loaders, please print one sample batch of  features and labels from the training set.\n",
        "\n",
        "Please use the batch_size provided below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA1tW3BsPKeg"
      },
      "source": [
        "batch_size = 50\n",
        "\n",
        "################################################\n",
        "##### TODO CODE HERE\n",
        "#####   First instantiate a ReviewDataset class\n",
        "#####   Then call the data loader function\n",
        "#####   Then invoke an iterator on the train data loader\n",
        "dset = raise NotImplementedError\n",
        "train_loader, valid_loader, test_loader = raise NotImplementedError\n",
        "sample_x, sample_y = raise NotImplementedError\n",
        "################################################\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz_RIGBkPQGf"
      },
      "source": [
        "# First checking if GPU is available\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3XBm48FO_XS"
      },
      "source": [
        "### Q2.2 LSTM implementation [20 points]\n",
        "\n",
        "We will implement an [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) in this problem. \n",
        "\n",
        "We have already provided functions for loading the training data. Please define your LSTM model in the class `LSTM`.\n",
        "\n",
        "Feel free to change the paramenters or code if needed (we recommend that you do not change the signature of the methods). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsSDH3aCPSqP"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        ################################################\n",
        "        ##### TODO CODE HERE\n",
        "        ##### Declare your parameters for various gates\n",
        "        raise NotImplementedError\n",
        "        ################################################\n",
        "\n",
        "        ################################################\n",
        "        ##### TODO CODE HERE\n",
        "        ##### Initialize all the paramters with nn.init.kaiming_normal_\n",
        "        ##### NOTE : initialize biases as 0.0\n",
        "        for param in self.parameters():\n",
        "            raise NotImplementedError\n",
        "        ################################################\n",
        "                \n",
        "    def forward(self, inputs, state=None):\n",
        "        # Argument: \n",
        "        #     input : (batch_size, seq_length, input_size)\n",
        "        # Returns:\n",
        "        #     output : (batch_size, seq_length, hidden_size)\n",
        "        #     hidden : ( (batch_size, hidden_size), (batch_size, hidden_size) )\n",
        "        \n",
        "        if state is None:\n",
        "            state = ( inputs.new_zeros(x.shape[0], self.hidden_size), \n",
        "                      inputs.new_zeros(x.shape[0], self.hidden_size) )\n",
        "\n",
        "        h, c = state\n",
        "        outputs = []\n",
        "        for input in torch.unbind(inputs, dim=1):\n",
        "\n",
        "            ################################################\n",
        "            ##### TODO CODE HERE\n",
        "            ##### Use input, and previous (h,c) to update (h,c) for this timestep\n",
        "            c = raise NotImplementedError\n",
        "            h = raise NotImplementedError\n",
        "            ################################################\n",
        "\n",
        "            outputs.append( h )\n",
        "            \n",
        "        output = torch.stack( outputs, dim=0 )\n",
        "        return output, (h, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGwzdgi3PVR-"
      },
      "source": [
        "Below we define the `SentimentRNN` class which utilizes the `LSTM` defined above as well as introduces a word embedding and a logistic classifier. \n",
        "\n",
        "Please go through this module in order to understand the sentiment classifier and its building blocks.\n",
        "\n",
        "Please do not modify this code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLArcW7zPXOp"
      },
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform \"sentiment analysis\".\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = LSTM(embedding_dim, hidden_dim)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        # linear and sigmoid layer\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        \n",
        "        # stack up lstm outputs\n",
        "        lstm_out = hidden[0] \n",
        "        \n",
        "        # dropout and fully connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size, cuda=True):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if(cuda):\n",
        "          hidden = (weight.new(batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                   weight.new(batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "          hidden = (weight.new(batch_size, self.hidden_dim).zero_(),\n",
        "                   weight.new(batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdCNvnA6PYR2"
      },
      "source": [
        "### Q2.3 Set up optimization [10 points]\n",
        "\n",
        "We instantiate the `SentimentRNN` module and ask you to specify the loss function as well as the optimizer for the learning task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkU_CJJJPZ93"
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(dset.vocab_to_int) + 1 # +1 for zero padding + our word tokens\n",
        "embedding_dim = 400 \n",
        "hidden_dim = 256\n",
        "\n",
        "net = SentimentRNN(vocab_size, embedding_dim, hidden_dim)\n",
        "print(net)\n",
        "\n",
        "# loss and optimization functions\n",
        "################################################\n",
        "##### TODO CODE HERE\n",
        "##### Note that we are dealing with binary classification (so choose the loss function appropriately)\n",
        "##### Also, specify the optimizer (feel free to use any)\n",
        "criterion = raise NotImplementedError\n",
        "optimizer = raise NotImplementedError\n",
        "\n",
        "# Set epochs to be the epoch number where validation loss stop decreasing\n",
        "epochs = 4 \n",
        "################################################\n",
        "\n",
        "# training params\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        ################################################\n",
        "        ##### TODO CODE HERE\n",
        "        #####     1) find out loss value (between the output and labels)\n",
        "        #####     2) perform backpropagation\n",
        "        #####     3) try using nn.utils.clip_grad_norm_ to clip gradients \n",
        "        #####         in order to avoid exploding gradients (common issue with LSTMS)\n",
        "        #####     4) take optimizer step \n",
        "        #####      \n",
        "        raise NotImplementedError\n",
        "        ################################################\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQLVKAJyPx8K"
      },
      "source": [
        "### Q2.4 Evaluation [5 points]\n",
        "\n",
        "We will evaluate the performance of the trained sentiment classifier below.\n",
        "There are two aspects to evaluation \n",
        "\n",
        "1. Accuracy on the test data\n",
        "2. Qualitative results on the given positive and negative reviews.\n",
        "\n",
        "If you have been able to train the network correctly, you should easily achieve 80% accuracy on test data and should be easily able to detect the correct sentiments for the provided reviews.\n",
        "\n",
        "Please run the following two blocks of data and leave the results as it is in the submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcWJaUdOPz--"
      },
      "source": [
        "# Get test data loss and accuracy\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = 100.0*(  num_correct/len(test_loader.dataset) )\n",
        "print(\"Test accuracy: {:.2f}%\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LzG41RQP3J2"
      },
      "source": [
        "def predict(net, test_review, sequence_length=200):\n",
        "    ''' Prints out whether a give review is predicted to be \n",
        "        positive or negative in sentiment, using a trained model.\n",
        "        \n",
        "        params:\n",
        "        net - A trained net \n",
        "        test_review - a review made of normal text and punctuation\n",
        "        sequence_length - the padded length of a review\n",
        "        '''\n",
        "    \n",
        "    net.eval()\n",
        "    \n",
        "    feature_tensor = dset.tokenize_review(test_review)\n",
        "\n",
        "    batch_size = feature_tensor.size(0)\n",
        "    \n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "      feature_tensor = feature_tensor.cuda()\n",
        "      \n",
        "    # get the output from the model\n",
        "    output, h = net(feature_tensor, h)\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())\n",
        "    # printing output value, before rounding\n",
        "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
        "    \n",
        "    # print custom response based on whether test_review is pos/neg\n",
        "    if(pred.item()==1):\n",
        "      print('Positive review detected!')\n",
        "    else:\n",
        "      print('Negative review detected!')\n",
        "    \n",
        "# call function\n",
        "# try negative and positive reviews!\n",
        "review_pos, review_neg = dset.get_positive_and_negative_reviews()\n",
        "\n",
        "# Prediction should be negative sentiment\n",
        "predict(net, review_neg, dset.seq_length)\n",
        "\n",
        "# Prediction should be positive sentiment\n",
        "predict(net, review_pos, dset.seq_length) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi43Y6hcP5C0"
      },
      "source": [
        "### Q2.5 Bonus: Improve the classifier [20 points]\n",
        "\n",
        "The 80% test performance on this task is not so great. Try improving the performance with various tools \n",
        "\n",
        "1. implementing a bi-directional LSTM\n",
        "2. use multiple LSTM layers for the classifier\n",
        "3. incorporate some form of regularization in the loss or dropout in the LSTM variables\n",
        "\n",
        "Please create new LSTM class for such implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTwAor1_P7CA"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9_y1s7OP7pw"
      },
      "source": [
        "## Q3 : GAN model on Celeb-A face dataset (40 points)\n",
        "\n",
        "We will implement a Generative Adversarial Network (GAN) in Q3 and Q4. In this problem, we will start by implementing basic helper functions and a working training routine which would be modified a bit in problem Q4 to present a hairstyle change application. \n",
        "\n",
        "This assignment is inspired by the following research paper, \n",
        "\n",
        "[StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation](https://arxiv.org/abs/1711.09020)\n",
        "\n",
        "Note that the basic building blocks of GAN are \n",
        "1. **Generator** : generates images similar to the real images provided in the dataset in order to fool the discriminator.\n",
        "\n",
        "2. **Discriminator** : acts as a fact checker in order to determine which images are fake and which are real \n",
        "\n",
        "3. **Latent representation** : generator cannot arbitrarily generate images out of thin air. It picks up a latent representation (usually a noise or some fixed pattern), and utilizes this source as a latent code and generates image during this process.\n",
        "\n",
        "For an easy tutorial to understand GAN's building blocks, please follow the PyTorch DC-GAN [tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html). It is recommended you study the tutorial before proceeding with the assignment. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1BFi66YQBR0"
      },
      "source": [
        "Note that f and g in the following image are the same as $D$ and $G$ respectively"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7C8TYk_QDxq"
      },
      "source": [
        "<img src=\"https://i.imgur.com/FhSycJD.png\" style=\"width: 600px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2w46fbUQEsP"
      },
      "source": [
        "Although the tutorial describes the GAN framework very well with appropriate links to the original paper, for completeness, we mention the losses for the discriminator and generator. The GAN optimization objective is the following : \n",
        "\n",
        "$$\\underset{G}{\\text{min}} ~~ \\underset{D}{\\text{max}} ~~ V(D,G) = \\mathbb{E}_{x\\sim p_{x}(x)}\\big[\\log D(x)\\big] + \\mathbb{E}_{z\\sim p_{z}(z)}\\big[\\log (1-D(G(z)))\\big]$$\n",
        "\n",
        "where $p_{x}(x)$ is the distribution of the real image data and $p_{z}(z)$ is the distribution from which latent vectors are sampled to input to the generator. $G$ is the generator, parametrized by parameters $w$ and $D$, the discriminator with parameters $\\phi$. In practice, these parameters are usually optimized in an alternating fashion, fixing one when optimizing the other, with the following loss functions:\n",
        "\n",
        "$$ loss_D(\\phi) = - \\mathbb{E}_{x\\sim p_{x}(x)}\\big[\\log D(x; \\phi)\\big] - \\mathbb{E}_{z\\sim p_{z}(z)}\\big[\\log (1-D(G(z; w); \\phi))\\big] $$\n",
        "\n",
        "$$ loss_G(w) = - \\mathbb{E}_{z\\sim p_{z}(z)}\\big[\\log (D(G(z; w); \\phi))\\big] $$\n",
        "\n",
        "In this assignment you shall use these loss functions and method of training $G$ and $D$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWjTwavYQH-C"
      },
      "source": [
        "### Arguments\n",
        "\n",
        "First, we first define some arguments for the training run. \n",
        "\n",
        "-  **selected_attributes** -  the attributes which will be used for generating various style attributes on the celebrity face.\n",
        "-  **c_dim** - the number of attributes we will use from the CelebA dataset. its set to `len(selected_attributes)`\n",
        "-  **image_size** - the spatial size of the images used for training.\n",
        "   This implementation defaults to 64x64. \n",
        "-  **g_conv_dim** - the number of convolutional fitlers for generator\n",
        "-  **d_conv_dim** - the number of convolutional fitlers for discriminator \n",
        "-  **g_repeat_num** - the number of residual blocks in the generator \n",
        "-  **d_repeat_num** - the number of residual blocks in the discriminator\n",
        "-  **lambda_cls** - the regularization hyper-parameter for classification error\n",
        "-  **lambda_rec** -  the regularization hyper-parameter for reconstruction error\n",
        "-  **lambda_gp** -  the regularization hyper-parameter for gradient penalty\n",
        "\n",
        "-  **batch_size** - the batch size used in training. The paper\n",
        "   uses a batch size of 16 for large configuration\n",
        "-  **num_iters** - the number of training iterations\n",
        "   the DataLoader\n",
        "-  **num_iters_decay** - the number of iterations after which learning rate will decay\n",
        "-  **g_lr** - the learning rate for generator\n",
        "-  **d_lr** - the learning rate for discriminator\n",
        "-  **n_critic** - generator will be updated every n_critic iterations.\n",
        "-  **beta1** - beta1 hyperparameter for Adam optimizers. As described in\n",
        "   paper, this number should be 0.5\n",
        "-  **beta2** - beta2 hyperparameter for Adam optimizers. As described in\n",
        "   paper, this number should be 0.999\n",
        "-  **num_workers** - the number of worker threads for loading the data with\n",
        "   the DataLoader\n",
        "-  **log_step** - log the update every log_steps\n",
        "-  **sample_step** - generate a new sample every sample_step\n",
        "-  **model_save_step** - save the model every model_save_step\n",
        "-  **lr_update_step** - update the learning rate every lr_update_step\n",
        "-  **log_dir** - directory where logs are stored\n",
        "-  **sample_dir** - directory where samples are stored\n",
        "-  **model_save_dir** - directory where trained models are stored\n",
        "-  **result_dir** - directory where results are stored\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOK2A1FhQKrO"
      },
      "source": [
        "cudnn.benchmark = True\n",
        "\n",
        "def get_experiment_configuration(repeat_num=6, num_iters=200000,\n",
        "              log_step=100, sample_step=100, model_save_step=10000, \n",
        "              lr_update_step=1000, batch_size=16, mode='train', resume_iters=False,\n",
        "              selected_attributes = ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young']):\n",
        "    config = {}\n",
        "\n",
        "    # Model configurations.\n",
        "    config['c_dim'] = len(selected_attributes)\n",
        "    config['image_size'] = 64\n",
        "    config['g_conv_dim'] = 64\n",
        "    config['d_conv_dim'] = 64\n",
        "    config['g_repeat_num'] = repeat_num\n",
        "    config['d_repeat_num'] = repeat_num\n",
        "    config['lambda_cls'] = 1\n",
        "    config['lambda_rec'] = 10\n",
        "    config['lambda_gp'] = 10\n",
        "    config['selected_attributes'] = selected_attributes \n",
        "\n",
        "    # Training configurations.\n",
        "    config['batch_size'] = batch_size #16\n",
        "    config['num_iters'] = num_iters\n",
        "    config['num_iters_decay'] = num_iters//2\n",
        "    config['g_lr'] = 0.0001\n",
        "    config['d_lr'] = 0.0001\n",
        "    config['n_critic'] = 5\n",
        "    config['beta1'] = 0.5\n",
        "    config['beta2'] = 0.999\n",
        "    config['resume_iters'] = resume_iters\n",
        "\n",
        "    # Test configurations.\n",
        "    config['test_iters'] = num_iters\n",
        "\n",
        "    # Miscellaneous.\n",
        "    config['device'] = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    config['num_workers'] = 1\n",
        "    config['mode'] = mode\n",
        "\n",
        "    # Step size.\n",
        "    config['log_step'] = log_step #10\n",
        "    config['sample_step'] = sample_step\n",
        "    config['model_save_step'] =  model_save_step #10000\n",
        "    config['lr_update_step'] = lr_update_step # 1000\n",
        "\n",
        "    EXPERIMENT_RESULTS_FOLDER = EXPERIMENTS_DIRECTORY + 'gan-experiments/'\n",
        "\n",
        "    suffix = str(repeat_num) + '-cdim-' + str(len(selected_attributes))\n",
        "    config['log_dir'] = EXPERIMENT_RESULTS_FOLDER + 'logs-' + suffix\n",
        "    config['sample_dir'] = EXPERIMENT_RESULTS_FOLDER + 'sample_dir-' + suffix\n",
        "    config['model_save_dir'] = EXPERIMENT_RESULTS_FOLDER + 'model_save_dir-' + suffix\n",
        "    config['result_dir'] = EXPERIMENT_RESULTS_FOLDER + 'result_dir-' + suffix\n",
        "\n",
        "    print('\\n\\nPlease ensure you are using a GPU for computation')\n",
        "    print('Will be using the following device for computation : ', config['device'])\n",
        "\n",
        "    # Create directories if not exist.\n",
        "    if not os.path.exists(config['log_dir']):\n",
        "        os.makedirs(config['log_dir'])\n",
        "    if not os.path.exists(config['sample_dir']):\n",
        "        os.makedirs(config['sample_dir'])\n",
        "    if not os.path.exists(config['model_save_dir']):\n",
        "        os.makedirs(config['model_save_dir'])\n",
        "    if not os.path.exists(config['result_dir']):\n",
        "        os.makedirs(config['result_dir'])\n",
        "\n",
        "    return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-Pj8Z_6QIm1"
      },
      "source": [
        "### Data\n",
        "\n",
        "We have downloaded pre-processed and stored data in the HDF5 format on the zip file provided with the assignment. Think of the file storing a large numpy ndarray of images (shape : `num_imgs x num_channels x height x width`). `celebA` class implemented below derives from `torch.utils.data.Dataset` and provides the code infrastucture to read images from this file. \n",
        "\n",
        "This data contains images of many celebrities along with labels for various image attributes (hair, gender, age, etc). There are 40 such attributes. We will use them later for cool applications. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvljBNDvQNGx"
      },
      "source": [
        "ALL_ATTRIBUTES = ['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive',  \n",
        "      'Bags_Under_Eyes',  'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', \n",
        "      'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby',\n",
        "      'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup',\n",
        "      'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', 'Mustache', \n",
        "      'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', \n",
        "      'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair',\n",
        "      'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick',\n",
        "      'Wearing_Necklace', 'Wearing_Necktie', 'Young' ]\n",
        "print('# attributes = ', len(ALL_ATTRIBUTES))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqsN-2NiQPS0"
      },
      "source": [
        "class CelebA(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset class for the CelebA dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, transform, mode, config):\n",
        "        \"\"\"Initialize and preprocess the CelebA dataset.\"\"\"\n",
        "\n",
        "        self.file = h5py.File(CELEBA_GOOGLE_DRIVE_PATH, 'r')\n",
        "        self.total_num_imgs, self.H, self.W, self.C = self.file['images'].shape\n",
        "\n",
        "        self.images = self.file['images']\n",
        "        self.attributes = self.file['attributes']\n",
        "\n",
        "        self.selected_attrs = config['selected_attributes'] \n",
        "        self.all_attr_names = ALL_ATTRIBUTES\n",
        "\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "\n",
        "        self.train_dataset = []\n",
        "        self.test_dataset = []\n",
        "        self.attr2idx = {}\n",
        "        self.idx2attr = {}\n",
        "        self.preprocess()\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.num_images = len(self.train_dataset)\n",
        "        else:\n",
        "            self.num_images = len(self.test_dataset)\n",
        "\n",
        "    def preprocess(self):\n",
        "        \"\"\"Preprocess the CelebA attribute file.\"\"\"\n",
        "        for i, attr_name in enumerate(self.all_attr_names):\n",
        "            self.attr2idx[attr_name] = i\n",
        "            self.idx2attr[i] = attr_name\n",
        "\n",
        "        self.all_idxs = np.arange(self.total_num_imgs)\n",
        "        N_test = 9\n",
        "        self.train_dataset = self.all_idxs[:-N_test] \n",
        "        self.test_dataset = self.all_idxs[-N_test:]\n",
        "\n",
        "        random.seed(1234)\n",
        "        np.random.seed(1234)        \n",
        "        np.random.shuffle(self.train_dataset)\n",
        "\n",
        "        print('Finished preprocessing the CelebA dataset...')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Return one image and its corresponding attribute label.\"\"\"\n",
        "        dataset = self.train_dataset if self.mode == 'train' else self.test_dataset\n",
        "        idx = dataset[index]\n",
        "\n",
        "        image = self.file['images'][idx]\n",
        "        attributes = self.file['attributes'][idx]\n",
        "\n",
        "        label = []\n",
        "        for attr_name in self.selected_attrs:\n",
        "            idx = self.attr2idx[attr_name]\n",
        "            label.append(attributes[idx])\n",
        "        \n",
        "        return self.transform(image), torch.FloatTensor(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of images.\"\"\"\n",
        "        return self.num_images\n",
        "\n",
        "\n",
        "def get_loader(config, mode='train'):\n",
        "    \"\"\"Build and return a data loader.\"\"\"\n",
        "    \n",
        "    batch_size = config['batch_size']\n",
        "    num_workers = config['num_workers']\n",
        "    \n",
        "    transform = []\n",
        "    transform.append(T.ToPILImage())\n",
        "    if mode == 'train':\n",
        "        transform.append(T.RandomHorizontalFlip())\n",
        "    transform.append(T.ToTensor())\n",
        "    transform.append(T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))\n",
        "    transform = T.Compose(transform)\n",
        "    \n",
        "    dataset = CelebA(transform, mode, config)\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=(mode=='train'),\n",
        "                                  num_workers=num_workers)\n",
        "    return data_loader\n",
        "  \n",
        "def denorm(x):\n",
        "    \"\"\"Convert the range from [-1, 1] to [0, 1].\"\"\"\n",
        "    out = (x + 1) / 2\n",
        "    return out.clamp_(0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF4rhou1QQ2G"
      },
      "source": [
        "SELECTED_ATTRIBUTES = ['Black_Hair', 'Blond_Hair', 'Brown_Hair']\n",
        "\n",
        "small_config = get_experiment_configuration(repeat_num=1, num_iters=20000,\n",
        "              batch_size=128, selected_attributes = SELECTED_ATTRIBUTES)\n",
        "\n",
        "loader = get_loader(small_config, mode='test')\n",
        "data_iter = iter(loader)\n",
        "x_fixed, _ = next(data_iter)\n",
        "\n",
        "from torchvision.transforms import ToPILImage\n",
        "to_img = ToPILImage()\n",
        "\n",
        "# display tensor\n",
        "to_img( denorm( x_fixed[0]  ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHlF8-VVQSqI"
      },
      "source": [
        "### Modules for Generator and Discriminator\n",
        "\n",
        "The following cell defines the generator and discriminator networks as `nn.Modules` ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0nZM5PpQUOK"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual Block with instance normalization.\"\"\"\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.main(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"Generator network.\"\"\"\n",
        "    def __init__(self, conv_dim=64, c_dim=5, repeat_num=6):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(3+c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False))\n",
        "        layers.append(nn.InstanceNorm2d(conv_dim, affine=True, track_running_stats=True))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        # Down-sampling layers.\n",
        "        curr_dim = conv_dim\n",
        "        for i in range(2):\n",
        "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False))\n",
        "            layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True, track_running_stats=True))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        # Bottleneck layers.\n",
        "        for i in range(repeat_num):\n",
        "            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))\n",
        "\n",
        "        # Up-sampling layers.\n",
        "        for i in range(2):\n",
        "            layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=4, stride=2, padding=1, bias=False))\n",
        "            layers.append(nn.InstanceNorm2d(curr_dim//2, affine=True, track_running_stats=True))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            curr_dim = curr_dim // 2\n",
        "\n",
        "        layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False))\n",
        "        layers.append(nn.Tanh())\n",
        "        self.main = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        # Replicate spatially and concatenate domain information.\n",
        "        # Note that this type of label conditioning does not work at all if we use reflection padding in Conv2d.\n",
        "        # This is because instance normalization ignores the shifting (or bias) effect.\n",
        "        c = c.view(c.size(0), c.size(1), 1, 1)\n",
        "        c = c.repeat(1, 1, x.size(2), x.size(3))\n",
        "        x = torch.cat([x, c], dim=1)\n",
        "        return self.main(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"Discriminator network.\"\"\"\n",
        "    def __init__(self, image_size=128, conv_dim=64, c_dim=5, repeat_num=6):\n",
        "        super(Discriminator, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(3, conv_dim, kernel_size=4, stride=2, padding=1))\n",
        "        layers.append(nn.LeakyReLU(0.01))\n",
        "\n",
        "        curr_dim = conv_dim\n",
        "        for i in range(1, repeat_num):\n",
        "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1))\n",
        "            layers.append(nn.LeakyReLU(0.01))\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        kernel_size = int(image_size / np.power(2, repeat_num))\n",
        "        self.main = nn.Sequential(*layers)\n",
        "        self.conv1 = nn.Conv2d(curr_dim, 1, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size, bias=False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = self.main(x)\n",
        "        out_src = self.conv1(h)\n",
        "        out_cls = self.conv2(h)\n",
        "        return out_src, out_cls.view(out_cls.size(0), out_cls.size(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBsEL946QXQp"
      },
      "source": [
        "Miscellaneous functions for updating learning rates, resetting the gradients and restoring a trained model from storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oagjs8fjQWaW"
      },
      "source": [
        "def update_lr(g_optimizer, d_optimizer, g_lr, d_lr):\n",
        "    \"\"\"Decay learning rates of the generator and discriminator.\"\"\"\n",
        "    for param_group in g_optimizer.param_groups:\n",
        "        param_group['lr'] = g_lr\n",
        "    for param_group in d_optimizer.param_groups:\n",
        "        param_group['lr'] = d_lr\n",
        "\n",
        "def reset_grad(g_optimizer, d_optimizer):    \n",
        "    g_optimizer.zero_grad()\n",
        "    d_optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4B9d3VyQaYz"
      },
      "source": [
        "### Helper Functions\n",
        "\n",
        "Implement the following helper functions to complete the training code. Follow the instructions in the questions below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adVQ8cA_QbJn"
      },
      "source": [
        "\n",
        "### Q3.1  Print number of parameters in the networks (5 points)\n",
        "\n",
        "Write a function that takes input as a model and the model name, and prints the model and the number of parameters in the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJL2sFyIQdYM"
      },
      "source": [
        "def print_network(model, name):\n",
        "  \"\"\"Print out the network information.\"\"\"\n",
        "  num_params = 0\n",
        "\n",
        "  ################################################\n",
        "  ##### TODO CODE HERE\n",
        "  raise NotImplementedError\n",
        "  ################################################\n",
        "\n",
        "  print(\"The number of parameters: {}\".format(num_params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-VXFVYPQgkB"
      },
      "source": [
        "### Q3.2 Invoke the optimizers on the Generator and Discriminator parameters (5 points)\n",
        "Write a function that takes optimization parameters as input and returns optimizer functions for the discriminator and generator in PyTorch. Use [ADAM](https://arxiv.org/pdf/1412.6980.pdf) optimizer with the parameters $\\beta_1$ and $\\beta_2$ specified earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFdZNMCaQiAQ"
      },
      "source": [
        "def get_optimizers(G, D, g_learning_rate, d_learning_rate, beta1, beta2):\n",
        "    \"\"\"\n",
        "    Returns a 2-tuple, optimizers for parameters of netD and netG\n",
        "    \"\"\"\n",
        "\n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    g_optimizer = raise NotImplementedError\n",
        "    d_optimizer = raise NotImplementedError\n",
        "    ################################################\n",
        "\n",
        "    return g_optimizer, d_optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUzYlXcKQkLE"
      },
      "source": [
        "### Q3.3 Compute classification loss (5 points)\n",
        "\n",
        "Given the logits and the target labels, compute binary cross entropy loss $\\ell_{\\text{cls}}$ ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moFjnMzNQk4B"
      },
      "source": [
        "def classification_loss(logit, target):\n",
        "    \"\"\"\n",
        "    Compute binary cross entropy loss.\n",
        "    \"\"\"\n",
        "\n",
        "    loss = 0.0\n",
        "\n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    loss = raise NotImplementedError\n",
        "    ################################################\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOxX4dbRQol0"
      },
      "source": [
        "### Q3.4 Compute reconstruction loss (5 points)\n",
        "\n",
        "This is a very popular loss function used in situations where you are given an original input $x$. In generative learning, through some latent space, you'll generate an almost replica of $x$, let us denote it by $\\hat{x}$.\n",
        "\n",
        "The reconstruction loss measures the distance between the replica and the original. Let $N$ be the number of elements in $x$ and $\\hat{x}$, then the loss can be written as \n",
        "\n",
        "$$\n",
        "\\ell_{\\text{rec}} =  \\frac{1}{N} \\sum^N_{i=1} | x_i - \\hat{x}_i |\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34_XK4pOQqsh"
      },
      "source": [
        "def reconstruction_loss( x_real, x_reconstructed ):\n",
        "    \"\"\"\n",
        "    Compute the reconstruction loss.\n",
        "    \"\"\"\n",
        "\n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    loss = raise NotImplementedError\n",
        "    ################################################\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVE8i3_mQ6OB"
      },
      "source": [
        "### Q3.5 Implement the discriminator loss (5 points)\n",
        "Write a function that returns the discriminator loss written as :\n",
        "\n",
        "$$\n",
        "\\ell_{discriminator} = \\ell_{real} + \\ell_{fake}  +  \\lambda_{cls} * \\ell_{cls}\n",
        "$$\n",
        "\n",
        "Note that we add an additional term associated with the generated data (its simple to compute so we do it for you :P )."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SqLMA4VQ98P"
      },
      "source": [
        "def get_discriminator_loss(G, D, label_org, x_real, c_trg, lambda_cls, lambda_gp):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    out_src, out_cls = D(x_real)\n",
        "    d_loss_real = -torch.mean(out_src)\n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    ##### classification loss between out_cls, label_org\n",
        "    d_loss_cls = raise NotImplementedError\n",
        "    ################################################\n",
        "\n",
        "    # Compute loss with fake images.\n",
        "    x_fake = G(x_real, c_trg)\n",
        "    out_src, out_cls = D(x_fake.detach())\n",
        "    d_loss_fake = torch.mean(out_src)\n",
        "\n",
        "    d_loss_gp = 0\n",
        "\n",
        "    # Backward and optimize.\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    ##### add remaining loss terms as described in the problem\n",
        "    d_loss += raise NotImplementedError\n",
        "    ################################################\n",
        "    \n",
        "    return d_loss, d_loss_real, d_loss_fake, d_loss_cls, d_loss_gp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHuGhQHhRAzx"
      },
      "source": [
        "### Q3.6 Implement the generator loss (5 points)\n",
        "\n",
        "Write a function that returns the generator loss written as :\n",
        "\n",
        "$$\n",
        "\\ell_{generator} = \\ell_{fake} + \\lambda_{rec} * \\ell_{rec} +  \\lambda_{cls} * \\ell_{cls}\n",
        "$$\n",
        "\n",
        "Note that we add an additional term associated with the generated data (its simple to compute so we do it for you :P )."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIjmvfYaRBlt"
      },
      "source": [
        "def get_generator_loss(G, D, x_real, c_trg, c_org, label_trg, lambda_rec, lambda_cls):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    # Original-to-target domain.\n",
        "    x_fake = G(x_real, c_trg)\n",
        "    out_src, out_cls = D(x_fake)\n",
        "    g_loss_fake = - torch.mean(out_src)\n",
        "\n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    ##### classification loss between out_cls and label_trg\n",
        "    g_loss_cls = raise NotImplementedError\n",
        "    ################################################\n",
        "\n",
        "    # Target-to-original domain.\n",
        "    x_reconst = G(x_fake, c_org)\n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    ##### reconstruction loss between x_real and x_reconst\n",
        "    g_loss_rec = raise NotImplementedError\n",
        "    ################################################\n",
        "\n",
        "    # Backward and optimize.\n",
        "    g_loss = g_loss_fake\n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    ##### add remaining loss terms as described in the problem\n",
        "    g_loss += raise NotImplementedError\n",
        "    ################################################\n",
        "\n",
        "    return g_loss, g_loss_fake, g_loss_cls, g_loss_rec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiI9JUv5REVO"
      },
      "source": [
        "The following cell initializes the generator and discriminator. Prints both  neural networks, and allocates optimizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Boq4dEKRFf3"
      },
      "source": [
        "# Instantiate Generator and Discriminator\n",
        "\n",
        "SELECTED_ATTRIBUTES = ['Blond_Hair']\n",
        "\n",
        "config = get_experiment_configuration(repeat_num=1, num_iters=10000, \n",
        "              log_step=100, sample_step=1000, model_save_step=1000, \n",
        "              batch_size=64, selected_attributes = SELECTED_ATTRIBUTES)\n",
        "\n",
        "G = Generator(config['g_conv_dim'], config['c_dim'], config['g_repeat_num'])\n",
        "\n",
        "D = Discriminator(config['image_size'], \n",
        "                  config['d_conv_dim'], \n",
        "                  config['c_dim'], \n",
        "                  config['d_repeat_num']) \n",
        "\n",
        "g_optimizer, d_optimizer = get_optimizers(G, D, \n",
        "                                          config['g_lr'], config['d_lr'], \n",
        "                                          config['beta1'], config['beta2'])\n",
        "\n",
        "print_network(G, 'G')\n",
        "print_network(D, 'D')\n",
        "    \n",
        "G = G.to(config['device'])\n",
        "D = D.to(config['device'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3sujVICRIS9"
      },
      "source": [
        "def create_labels(c_org, c_dim=5, selected_attrs=SELECTED_ATTRIBUTES):\n",
        "    \"\"\"Generate target domain labels for debugging and testing.\"\"\"\n",
        "    # Get hair color indices.\n",
        "    hair_color_indices = []\n",
        "    for i, attr_name in enumerate(selected_attrs):\n",
        "        if attr_name in ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair']:\n",
        "            hair_color_indices.append(i)\n",
        "\n",
        "    c_trg_list = []\n",
        "    for i in range(c_dim):\n",
        "        c_trg = c_org.clone()\n",
        "        if i in hair_color_indices:  # Set one hair color to 1 and the rest to 0.\n",
        "            c_trg[:, i] = 1\n",
        "            for j in hair_color_indices:\n",
        "                if j != i:\n",
        "                    c_trg[:, j] = 0\n",
        "        else:\n",
        "            c_trg[:, i] = (c_trg[:, i] == 0)  # Reverse attribute value.\n",
        "\n",
        "        c_trg_list.append(c_trg.to(config['device']))\n",
        "    return c_trg_list\n",
        "\n",
        "# Set data loader.\n",
        "data_loader = get_loader(config, 'train')\n",
        "device = config['device']\n",
        "\n",
        "# Fetch fixed inputs for debugging.\n",
        "data_iter = iter(data_loader)\n",
        "x_fixed, c_org = next(data_iter)\n",
        "x_fixed = x_fixed.to(device)\n",
        "c_fixed_list = create_labels(c_org, config['c_dim'], config['selected_attributes'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFEtsYS9RLVp"
      },
      "source": [
        "### Q3.7 : Training loop (10 points)\n",
        "Now, using the functions defined above, implement the main training loop. Some of it has already been done for you. Fill in code where indicated.\n",
        "\n",
        "Note that your after every `config['sample_step']`, the code generates new samples in the directory indicated in the configuration. Please monitor this to see how your generated images look like.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJD8P_mmRPl7"
      },
      "source": [
        "# Learning rate cache for decaying.\n",
        "g_lr = config['g_lr']\n",
        "d_lr = config['d_lr']\n",
        "\n",
        "# Start training from scratch or resume training.\n",
        "start_iters = 0\n",
        "\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "cur_g_loss = 0\n",
        "cur_d_loss = 0\n",
        "\n",
        "# Start training.\n",
        "print('Start training...')\n",
        "start_time = time.time()\n",
        "for i in range(start_iters, config['num_iters']):\n",
        "    # =================================================================================== #\n",
        "    #                             1. Preprocess input data                                #\n",
        "    # =================================================================================== #\n",
        "\n",
        "    # Fetch real images and labels.\n",
        "    try:\n",
        "        x_real, label_org = next(data_iter)\n",
        "    except:\n",
        "        data_iter = iter(data_loader)\n",
        "        x_real, label_org = next(data_iter)\n",
        "\n",
        "    # Generate target domain labels randomly.\n",
        "    rand_idx = torch.randperm(label_org.size(0))\n",
        "    label_trg = label_org[rand_idx]\n",
        "\n",
        "    c_org = label_org.clone()\n",
        "    c_trg = label_trg.clone()\n",
        "\n",
        "    x_real = x_real.to(device)           # Input images.\n",
        "    c_org = c_org.to(device)             # Original labels.\n",
        "    c_trg = c_trg.to(device)             # Target labels.\n",
        "    label_org = label_org.to(device)     # Labels for computing classification loss.\n",
        "    label_trg = label_trg.to(device)     # Labels for computing classification loss.\n",
        "\n",
        "    # Train discriminator\n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    ##### Get the discriminator loss and optimize discriminator\n",
        "    d_loss, d_loss_real, d_loss_fake, d_loss_cls, d_loss_gp = raise NotImplementedError\n",
        "    \n",
        "    # Now Optimize discriminator\n",
        "    ################################################\n",
        "\n",
        "    cur_d_loss = d_loss.item()\n",
        "    # Logging.\n",
        "    loss = {}\n",
        "    loss['D/loss_real'] = d_loss_real.item()\n",
        "    loss['D/loss_fake'] = d_loss_fake.item()\n",
        "    loss['D/loss_cls'] = d_loss_cls.item()\n",
        "    loss['D/loss_gp'] = d_loss_gp.item()\n",
        "    \n",
        "    # Train the generator                         \n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    ##### Get the generator loss and optimize generator\n",
        "    g_loss, g_loss_fake, g_loss_cls, g_loss_rec =  raise NotImplementedError     \n",
        "    \n",
        "    # Now Optimize generator\n",
        "    ################################################\n",
        "\n",
        "    # Logging.\n",
        "    loss['G/loss_fake'] = g_loss_fake.item()\n",
        "    loss['G/loss_rec'] = g_loss_rec.item()\n",
        "    loss['G/loss_cls'] = g_loss_cls.item()\n",
        "    cur_g_loss = g_loss.item() \n",
        "\n",
        "    # Save Losses for plotting later\n",
        "    G_losses.append(cur_g_loss)\n",
        "    D_losses.append(cur_d_loss)\n",
        "\n",
        "    # Print out training information.\n",
        "    if (i+1) % config['log_step']  == 0:\n",
        "        et = time.time() - start_time\n",
        "        et = str(datetime.timedelta(seconds=et))[:-7]\n",
        "        log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, i+1, config['num_iters'])\n",
        "        for tag, value in loss.items():\n",
        "            log += \", {}: {:.4f}\".format(tag, value)\n",
        "        print(log)\n",
        "\n",
        "    # Translate fixed images for debugging.\n",
        "    if (i+1) %  config['sample_step']  == 0:\n",
        "        with torch.no_grad():\n",
        "            x_fake_list = [x_fixed]\n",
        "            for c_fixed in c_fixed_list:\n",
        "                x_fake_list.append(G(x_fixed, c_fixed))\n",
        "            x_concat = torch.cat(x_fake_list, dim=3)\n",
        "            sample_path = os.path.join(config['sample_dir'], '{}-images.jpg'.format(i+1))\n",
        "            save_image(denorm(x_concat.data.cpu()), sample_path, nrow=1, padding=0)\n",
        "            print('Saved real and fake images into {}...'.format(sample_path))\n",
        "\n",
        "    # Decay learning rates.\n",
        "    if (i+1) % config['lr_update_step'] == 0 and (i+1) > (config['num_iters'] - config['num_iters_decay']):\n",
        "        g_lr -= (config['g_lr'] / float(config['num_iters_decay']))\n",
        "        d_lr -= (config['d_lr'] / float(config['num_iters_decay']))\n",
        "        update_lr(g_optimizer, d_optimizer, g_lr, d_lr)\n",
        "        print ('Decayed learning rates, g_lr: {}, d_lr: {}.'.format(g_lr, d_lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVXxjM7ORLP7"
      },
      "source": [
        "- Plot the generator and discriminator losses. Remember to leave this output intact when you submit the notebook. Not doing so would result in a 2 points penalty."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geJn6oOARSNA"
      },
      "source": [
        "# Losses\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00QYk7J-RWjk"
      },
      "source": [
        "## Q4: Hair-style transformation\n",
        "\n",
        "In this problem, we will take image and change hair styles as per the trained GAN in the previous step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3rTkwGiRXZZ"
      },
      "source": [
        "### Q4.1 : Save trained model (5 points)\n",
        "\n",
        "We will implement a routine to save the trained generator and discrimantor models, so that we can simply load these later on for inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfZ35sFxRZ_f"
      },
      "source": [
        "def save_model(G, D, config, step):\n",
        "    \"\"\"\n",
        "    Save the trained generator and discriminator\n",
        "    \"\"\"\n",
        "    model_save_dir = config['model_save_dir']\n",
        "    G_path = os.path.join(model_save_dir, '{}-G.ckpt'.format(step+1))\n",
        "    D_path = os.path.join(model_save_dir, '{}-D.ckpt'.format(step+1))\n",
        "\n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    raise NotImplementedError\n",
        "    ################################################\n",
        "    \n",
        "    print('Saved model checkpoints into {}...'.format(model_save_dir))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01w4Md9GRatn"
      },
      "source": [
        "### Q4.2 : Load trained model (5 points)\n",
        "\n",
        "We will implement a routine to load the trained generator and discrimantor models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG-GqisTRcYN"
      },
      "source": [
        "def restore_model(resume_iters, model_save_dir):\n",
        "    \"\"\"\n",
        "    Restore the trained generator and discriminator.\n",
        "    \"\"\"\n",
        "\n",
        "    print('Loading the trained models from step {}...'.format(resume_iters))\n",
        "    G_path = os.path.join(model_save_dir, '{}-G.ckpt'.format(resume_iters))\n",
        "    D_path = os.path.join(model_save_dir, '{}-D.ckpt'.format(resume_iters))\n",
        "\n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    raise NotImplementedError\n",
        "    ################################################\n",
        "\n",
        "    return G, D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EtNx4gSRevi"
      },
      "source": [
        "### Q4.3 Implement gradient penalty (10 points)\n",
        "\n",
        "Given $y = f(x)$, we refer $ \\frac{ dy }{ dx } $ as the gradient in this problem. We want to include a gradient penalty in the GAN loss. \n",
        "We can write gradient penalty $\\ell_{gp}$ as \n",
        "\n",
        "$$\n",
        "\\ell_{gp} = \\Bigg\\|  \\Big\\| \\frac{ dy }{ dx } \\Big\\|_2 - 1 \\Bigg\\|^2\n",
        "$$\n",
        "\n",
        "(Hint : Using ```grad``` function in the ```torch.autograd``` modules, compute the gradient penalty )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3fmCkpJRfZP"
      },
      "source": [
        "def gradient_penalty(y, x):\n",
        "    \"\"\"\n",
        "    Compute gradient penalty: (L2_norm(dy/dx) - 1)**2.\n",
        "    \"\"\"\n",
        "\n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    dydx_l2norm = raise NotImplementedError\n",
        "    ################################################\n",
        "\n",
        "    loss = torch.mean((dydx_l2norm-1)**2)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBWi2WH6RhhP"
      },
      "source": [
        "### Q4.4 Implement the discriminator loss (5 points)\n",
        "Write a function that returns the discriminator loss written as :\n",
        "\n",
        "$$\n",
        "\\ell_{discriminator} = \\ell_{real} + \\ell_{fake} + \\lambda_{gp} \\times \\ell_{gp} +  \\lambda_{cls} \\times \\ell_{cls}\n",
        "$$\n",
        "\n",
        "Note that we add an additional term associated with the generated data (its simple to compute so we do it for you :P )."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b29Aeko5RiMv"
      },
      "source": [
        "\n",
        "def get_new_discriminator_loss(G, D, label_org, x_real, c_trg, lambda_cls, lambda_gp):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    out_src, out_cls = D(x_real)\n",
        "    d_loss_real = -torch.mean(out_src)\n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    ##### classification loss between out_cls, label_org\n",
        "    d_loss_cls = raise NotImplementedError\n",
        "    ################################################\n",
        "\n",
        "    # Compute loss with fake images.\n",
        "    x_fake = G(x_real, c_trg)\n",
        "    out_src, out_cls = D(x_fake.detach())\n",
        "    d_loss_fake = torch.mean(out_src)\n",
        "\n",
        "    # Compute loss for gradient penalty.\n",
        "    alpha = torch.rand(x_real.size(0), 1, 1, 1).to(device)\n",
        "    x_hat = (alpha * x_real.data + (1 - alpha) * x_fake.data).requires_grad_(True)\n",
        "    out_src, _ = D(x_hat)\n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    ##### gradient penalty on y=out_src, x=x_hat\n",
        "    d_loss_gp = raise NotImplementedError\n",
        "    ################################################\n",
        "\n",
        "    # Backward and optimize.\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    ##### add remaining loss terms as described in the problem\n",
        "    d_loss += raise NotImplementedError\n",
        "    ################################################\n",
        "    \n",
        "    return d_loss, d_loss_real, d_loss_fake, d_loss_cls, d_loss_gp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fM-Z9liRkd8"
      },
      "source": [
        "### Q4.5 : Initialize a larger GAN using ```get_experiment_configuration``` (10 points)\n",
        "\n",
        "We will use three hair style attributes in this experiment.\n",
        "\n",
        "We will train a larger GAN in this problem. First, lets get a larger generator and discriminator models (use more than 3 repeat blocks in the experiment configuration which will increase the number of residual blocks in both the models).\n",
        "\n",
        "Initialize the generator and discriminator accordingly and get the optimizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEfUod3pRlu0"
      },
      "source": [
        "\n",
        "SELECTED_ATTRIBUTES = ['Black_Hair', 'Blond_Hair', 'Brown_Hair']\n",
        "\n",
        "################################################\n",
        "##### TODO CODE HERE\n",
        "raise NotImplementedError\n",
        "################################################\n",
        "\n",
        "print_network(G, 'G')\n",
        "print_network(D, 'D')\n",
        "    \n",
        "G = G.to(config['device'])\n",
        "D = D.to(config['device'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIVucY51RnZo"
      },
      "source": [
        "### Q4.6 : Train the larger GAN (15 points)\n",
        "\n",
        "At the heart of a GAN network is a minimax problem. Earlier we were optimizing the Generator and the Discriminator at the same speed. \n",
        "\n",
        "It turns out that in this case, its recommended that the generator is updated at a slower pace than the discriminator. \n",
        "\n",
        "One way to achieve this is to run the generator with smaller learning rate . Instead we want to use the same learning rates as before but we will update the generator every few iterations, i.e. discriminator will be trained in every iteration but the generator will be trained every 5 iterations or more specifically ```config['n_critic']``` number of iterations. Use this recommended value or update as per your intuition.\n",
        "\n",
        "Note that your after every `config['sample_step']`, the code generates new samples in the directory indicated in the configuration. Please monitor this to see how your generated images look like.\n",
        "\n",
        "The script also saves your model to the `config['model_save_dir']` every `config['mode_save_step']` so that you can resume training (in case your script crashes after making significant progress) and we can also restore this model when we generate new hairstyles on the test images.\n",
        "\n",
        "Hopefully your results will be better with these updates.\n",
        "\n",
        "This will take more than 10 hours to give you reasonable images. So start early.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1wLrKT8RoLK"
      },
      "source": [
        "# Learning rate cache for decaying.\n",
        "g_lr = config['g_lr']\n",
        "d_lr = config['d_lr']\n",
        "\n",
        "# Start training from scratch or resume training.\n",
        "start_iters = 0\n",
        "if config['resume_iters']:\n",
        "    start_iters = config['resume_iters'] \n",
        "    G, D = restore_model(config['resume_iters'], config )\n",
        "\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "cur_g_loss = 0\n",
        "cur_d_loss = 0\n",
        "\n",
        "# Start training.\n",
        "print('Start training...')\n",
        "start_time = time.time()\n",
        "for i in range(start_iters, config['num_iters']):\n",
        "    # =================================================================================== #\n",
        "    #                             1. Preprocess input data                                #\n",
        "    # =================================================================================== #\n",
        "\n",
        "    # Fetch real images and labels.\n",
        "    try:\n",
        "        x_real, label_org = next(data_iter)\n",
        "    except:\n",
        "        data_iter = iter(data_loader)\n",
        "        x_real, label_org = next(data_iter)\n",
        "\n",
        "    # Generate target domain labels randomly.\n",
        "    rand_idx = torch.randperm(label_org.size(0))\n",
        "    label_trg = label_org[rand_idx]\n",
        "\n",
        "    c_org = label_org.clone()\n",
        "    c_trg = label_trg.clone()\n",
        "\n",
        "    x_real = x_real.to(device)           # Input images.\n",
        "    c_org = c_org.to(device)             # Original labels.\n",
        "    c_trg = c_trg.to(device)             # Target labels.\n",
        "    label_org = label_org.to(device)     # Labels for computing classification loss.\n",
        "    label_trg = label_trg.to(device)     # Labels for computing classification loss.\n",
        "\n",
        "    # Train discriminator\n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    ##### Get the discriminator loss and optimize discriminator\n",
        "    d_loss, d_loss_real, d_loss_fake, d_loss_cls, d_loss_gp = raise NotImplementedError\n",
        "    \n",
        "    # Now Optimize discriminator\n",
        "    ################################################\n",
        "\n",
        "    cur_d_loss = d_loss.item()\n",
        "    # Logging.\n",
        "    loss = {}\n",
        "    loss['D/loss_real'] = d_loss_real.item()\n",
        "    loss['D/loss_fake'] = d_loss_fake.item()\n",
        "    loss['D/loss_cls'] = d_loss_cls.item()\n",
        "    loss['D/loss_gp'] = d_loss_gp.item()\n",
        "    \n",
        "    # Train the generator                         \n",
        "    ################################################\n",
        "    ##### TODO CODE HERE\n",
        "    ##### Get the generator loss and optimize generator (every n_critic iterations)\n",
        "    g_loss, g_loss_fake, g_loss_cls, g_loss_rec =  raise NotImplementedError     \n",
        "    \n",
        "    # Now Optimize generator\n",
        "    # Logging.\n",
        "    loss['G/loss_fake'] = g_loss_fake.item()\n",
        "    loss['G/loss_rec'] = g_loss_rec.item()\n",
        "    loss['G/loss_cls'] = g_loss_cls.item()\n",
        "    ################################################\n",
        "        \n",
        "    # Print out training information.\n",
        "    if (i+1) % config['log_step']  == 0:\n",
        "        et = time.time() - start_time\n",
        "        et = str(datetime.timedelta(seconds=et))[:-7]\n",
        "        log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, i+1, config['num_iters'])\n",
        "        for tag, value in loss.items():\n",
        "            log += \", {}: {:.4f}\".format(tag, value)\n",
        "        print(log)\n",
        "\n",
        "    # Translate fixed images for debugging.\n",
        "    if (i+1) %  config['sample_step']  == 0:\n",
        "        with torch.no_grad():\n",
        "            x_fake_list = [x_fixed]\n",
        "            for c_fixed in c_fixed_list:\n",
        "                x_fake_list.append(G(x_fixed, c_fixed))\n",
        "            x_concat = torch.cat(x_fake_list, dim=3)\n",
        "            sample_path = os.path.join(config['sample_dir'], '{}-images.jpg'.format(i+1))\n",
        "            save_image(denorm(x_concat.data.cpu()), sample_path, nrow=1, padding=0)\n",
        "            print('Saved real and fake images into {}...'.format(sample_path))\n",
        "\n",
        "    # Save model checkpoints.\n",
        "    if (i+1) % config['model_save_step'] == 0:\n",
        "        save_model(G, D, config, i)\n",
        "\n",
        "    # Decay learning rates.\n",
        "    if (i+1) % config['lr_update_step'] == 0 and (i+1) > (config['num_iters'] - config['num_iters_decay']):\n",
        "        g_lr -= (config['g_lr'] / float(config['num_iters_decay']))\n",
        "        d_lr -= (config['d_lr'] / float(config['num_iters_decay']))\n",
        "        update_lr(g_optimizer, d_optimizer, g_lr, d_lr)\n",
        "        print ('Decayed learning rates, g_lr: {}, d_lr: {}.'.format(g_lr, d_lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0908dwjGjgl"
      },
      "source": [
        "### Q4.7 : Generate new hairstyles for test images (10 points)\n",
        "\n",
        "We will apply the hairstyle transformations through the trained Generator model.\n",
        "We will restore the already trained model, load the test data and invoke the Generator with various hairstyle attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQjy63wE3b1i"
      },
      "source": [
        "You should expect the output to look like the following image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lenhlW-C3WOh"
      },
      "source": [
        "<img src=\"https://i.imgur.com/gbNzQk8.jpg\" style=\"width: 600px;\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT2d8uvPGlRM"
      },
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Choose the iteration number of the saved model (both G, D should be present)\n",
        "################################################\n",
        "##### TODO CODE HERE\n",
        "##### Load the trained model\n",
        "##### Also, load the data_loader in test mode\n",
        "G, D = raise NotImplementedError\n",
        "data_loader = raise NotImplementedError\n",
        "################################################\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (x_real, c_org) in enumerate(data_loader):\n",
        "\n",
        "        # Prepare input images and target domain labels.\n",
        "        x_real = x_real.to(config['device'])\n",
        "        c_trg_list = create_labels(c_org, config['c_dim'], config['selected_attributes'])\n",
        "\n",
        "        # Translate images.\n",
        "        x_fake_list = [x_real]\n",
        "        for c_trg in c_trg_list:\n",
        "            x_fake = G(x_real, c_trg)\n",
        "            x_fake_list.append(x_fake)\n",
        "\n",
        "        # Save the translated images.\n",
        "        x_concat = torch.cat(x_fake_list, dim=3)\n",
        "        result_path = os.path.join( config['result_dir'], '{}-images.jpg'.format(i+1) )\n",
        "        save_image(denorm(x_concat.data.cpu()), result_path, nrow=1, padding=0)\n",
        "        print('Saved real and fake images into {}...'.format(result_path))\n",
        "        display(Image(filename=result_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzZJG01OGocI"
      },
      "source": [
        "### Q4.8 : (Bonus) Use other attributes and develop something cool. (20 points)\n",
        "\n",
        "We have seen how to change hair style using GANs so far, but its possible to use any other attributes and develop something much cooler. You can use any other loss functions or the generator/discriminator architecture. Feel free to be creative and develop something cooler. "
      ]
    }
  ]
}